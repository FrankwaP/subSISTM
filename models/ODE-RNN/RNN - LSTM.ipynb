{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class: RNN\n",
    "class RNN(nn.Module):\n",
    "    #sizes are the number of nodes for respective layers, as ints\n",
    "    #input_size should be the numbers of variables in input, output_size the number of variables predicted\n",
    "    #hidden_size should be chosen after experimentation, since we only have one layer it should be more than the number of variables\n",
    "    def __init__(self, input_size, hidden_size, output_size, **kwds):\n",
    "        super().__init__(**kwds)\n",
    "        #Number of nodes of the hidden layer (used for init)\n",
    "        self.hidden_size = hidden_size\n",
    "        #Weights\n",
    "        self.i2h = nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    #Input must be torch.Tensor and normalized\n",
    "    def forward(self, input):\n",
    "        #h0 = self.initHidden()\n",
    "        h_list, hn = self.i2h(input)\n",
    "        output_list = self.h2o(h_list)\n",
    "        return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class : LSTM\n",
    "class LSTM(nn.Module):\n",
    "    #sizes are the number of nodes for respective layers, as ints\n",
    "    #input_size should be the numbers of variables in input, output_size the number of variables predicted\n",
    "    #hidden_size should be chosen after experimentation, since we only have one layer it should be more than the number of variables\n",
    "    def __init__(self, input_size, hidden_size, output_size, **kwds):\n",
    "        super().__init__(**kwds)\n",
    "        #Number of nodes of the hidden layer (used for init)\n",
    "        self.hidden_size = hidden_size\n",
    "        #Weights\n",
    "        self.i2h = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    \n",
    "    #Input must be torch.Tensor and normalized\n",
    "    def forward(self, input):\n",
    "        h_list, (hn,cn) = self.i2h(input)\n",
    "        output_list = self.h2o(h_list)\n",
    "        return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters to get several scenari\n",
    "CSV_FILE = \"../../data/synthetic_bph_1/Simulations/simulation1.csv\"\n",
    "TRAINING_NUMBER = 400\n",
    "USE_NOISY_DATA = False\n",
    "USE_MIXED_EFFECT = False\n",
    "timestep_skip = 1\n",
    "MODEL = RNN        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loading\n",
    "data = pd.read_csv(CSV_FILE, sep=\";\", decimal=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_labels = [\n",
    "    c for c in data.columns if c.startswith(\"x\") and ((\"_\" in c) is USE_NOISY_DATA)\n",
    "]\n",
    "if 'x8' not in x_labels:\n",
    "    x_labels.append('x8')\n",
    "#assert len(x_labels) == 8\n",
    "\n",
    "y_labels = [\n",
    "    c\n",
    "    for c in data.columns\n",
    "    if c.startswith(\"y\")\n",
    "    and ((\"_obs\" in c))\n",
    "    and ((\"_mixed\" in c) is USE_MIXED_EFFECT)\n",
    "]\n",
    "assert len(y_labels) == 1\n",
    "\n",
    "print(x_labels)\n",
    "print(y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "random.seed(0)\n",
    "data = data[data['temps']%timestep_skip == 0]\n",
    "data['y+1'] = data.groupby(\"individus\")[y_labels].shift(-1)\n",
    "data_norm = data.copy()\n",
    "data_norm = data_norm.dropna()\n",
    "N_train = random.sample(range(1,500), TRAINING_NUMBER)\n",
    "N_train.sort()\n",
    "data_train = data_norm.loc[data_norm['individus'].isin(N_train)]\n",
    "data_test = data_norm[~data_norm['individus'].isin(N_train)]\n",
    "\n",
    "scaler_x = MinMaxScaler()\n",
    "data_train.loc[:,x_labels + y_labels] = scaler_x.fit_transform(data_train[x_labels + y_labels])\n",
    "data_test.loc[:,x_labels + y_labels] = scaler_x.transform(data_test[x_labels + y_labels ])\n",
    "\n",
    "scaler_y = MinMaxScaler()\n",
    "data_train.loc[:,['y+1']] = scaler_y.fit_transform(data_train[['y+1']])\n",
    "data_test.loc[:,['y+1']] = scaler_y.transform(data_test[['y+1']])\n",
    "\n",
    "groupby = data_train.groupby('individus')[x_labels + y_labels].apply(np.array)\n",
    "input_train = [torch.Tensor(x) for x in groupby]\n",
    "input_train = torch.stack(input_train)\n",
    "groupby = data_train.groupby('individus')[['y+1']].apply(np.array)\n",
    "target_train = [torch.Tensor(x) for x in groupby]\n",
    "target_train = torch.stack(target_train)\n",
    "\n",
    "groupby = data_test.groupby('individus')[x_labels + y_labels].apply(np.array)\n",
    "input_test = [torch.Tensor(x) for x in groupby]\n",
    "input_test = torch.stack(input_test)\n",
    "groupby = data_test.groupby('individus')[['y+1']].apply(np.array)\n",
    "target_test = [torch.Tensor(x) for x in groupby]\n",
    "target_test = torch.stack(target_test)\n",
    "#seq_lens = [len(inp) for inp in groupby]\n",
    "#input_padded = pad_sequence([torch.tensor(x) for x in groupby])\n",
    "#input_pack_padded = pack_padded_sequence(input_padded, lengths=seq_lens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparamètres\n",
    "hidden_size = [25]\n",
    "learning_rate = [ 0.001]\n",
    "criterion = torch.nn.MSELoss()\n",
    "epoch = 1000\n",
    "eps = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training with convergence criteria and GridSearch\n",
    "min_loss = float('inf')\n",
    "for h in hidden_size:\n",
    "    for lr in learning_rate:\n",
    "        model = MODEL(input_size = 9, hidden_size=h, output_size=1)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "        loss_val = [float('inf')]*100\n",
    "        loss_train = []\n",
    "        cur_loss_val = 0\n",
    "        nb_epochs = 0\n",
    "        while abs(np.max(loss_val[-100:] - np.min(loss_val[-100:] + [cur_loss_val]))) >= eps:\n",
    "\n",
    "            y_test = model(input = input_test)\n",
    "            cur_loss_val = criterion(y_test, target_test).item()\n",
    "            loss_val.append(cur_loss_val)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(input=input_train)\n",
    "            loss = criterion(y_pred, target_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            cur_loss_train = loss.item()\n",
    "            loss_train.append(cur_loss_train)\n",
    "            \n",
    "            nb_epochs += 1\n",
    "        if cur_loss_val < min_loss:\n",
    "            min_loss = cur_loss_val\n",
    "            res = (h, lr)\n",
    "\n",
    "loss_val = loss_val[101:]\n",
    "print(h, lr)\n",
    "print(nb_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "#plt.plot(loss_track)\n",
    "plt.plot(loss_train, label =\"loss\")\n",
    "plt.plot(loss_val, label = \"validation\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(criterion(y_pred,target_train).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(N_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unscale data\n",
    "multi_index = pd.MultiIndex.from_product([N_train, range(y_pred.shape[1])], names=['individus', 'temps'])\n",
    "df = pd.DataFrame(index = multi_index, data = y_pred.detach().numpy().flatten(), columns=['y_pred'])\n",
    "df = df.reset_index()\n",
    "df = df.set_index(data_train.index)\n",
    "df.loc[:,'y_pred'] = scaler_y.inverse_transform(df[['y_pred']])\n",
    "data_train.loc[:,['y+1']] = scaler_y.inverse_transform(data_train[['y+1']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[(df['temps']!=0) & (df['individus'] == 1)]['y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results on training\n",
    "MSE_list = []\n",
    "for k in N_train:\n",
    "    pred_k = df[(df['temps']!=0) & (df['individus'] == k)]['y_pred']\n",
    "    target_k = data_train[(data_train['temps']!=0) & (data_train['individus'] == k)]['y+1']\n",
    "    MSE_list.append(mean_squared_error(pred_k, target_k))\n",
    "print(\"mean MSE on train data: \", np.mean(MSE_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot 4 random curves from training data and worst individual\n",
    "n = 5\n",
    "ex = randint(1,400, n-1)\n",
    "fig, axs = plt.subplots(n)\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(50)\n",
    "for k in range(n):\n",
    "    axs[k].plot(data_train.loc[(data_train['temps']!=0) & (data_train['individus']==k)]['y+1'], label= 'target')\n",
    "    axs[k].plot(df.loc[(df['temps']!=0) & (df['individus']==k)]['y_pred'], label= 'prediction')\n",
    "    axs[k].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE on test data\n",
    "y_pred = model(input_test)\n",
    "#Results\n",
    "MSE_list = []\n",
    "for k in range(len(y_pred)):\n",
    "    MSE_list.append(mean_squared_error(y_pred[k, 1:].detach().numpy(),target_test[k, 1:].detach().numpy()))\n",
    "print(\"mean MSE on test data: \", np.mean(MSE_list))\n",
    "print(\"Worst case :\", np.max(MSE_list), \"for individual \", np.argmax(MSE_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot n curves from test data\n",
    "n = 10\n",
    "ex = randint(1,100, n)\n",
    "fig, axs = plt.subplots(n)\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(50)\n",
    "res = []\n",
    "print(y_pred.shape)\n",
    "for k in range(n):\n",
    "    res.append((ex[k], mean_squared_error(target_test[ex[k], 1:].detach().numpy(), y_pred[ex[k], 1:].detach().numpy())))\n",
    "    axs[k].plot(target_test[ex[k], 1:].detach().numpy(), label= 'target')\n",
    "    axs[k].plot(y_pred[ex[k], 1:].detach().numpy(), label= 'prediction')\n",
    "    axs[k].plot(torch.roll(target_test, 1, 1)[ex[k], 1:], label = 'reporté de %timestepskip jours')\n",
    "    axs[k].legend()\n",
    "\n",
    "print(res) #res contient les MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot n worst curves\n",
    "indices = np.argpartition(MSE_list, -5)[-5:]\n",
    "n = 5\n",
    "ex = randint(1,100, n)\n",
    "fig, axs = plt.subplots(n)\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(50)\n",
    "for k in range(len(indices)):\n",
    "    axs[k].plot(target_test[indices[k], 1:].detach().numpy(), label= 'target')\n",
    "    axs[k].plot(y_pred[indices[k], 1:].detach().numpy(), label= 'prediction')\n",
    "    axs[k].plot(torch.roll(target_test, 1, 1)[indices[k], 1:], label = 'reporté de %timestepskip jours')\n",
    "    axs[k].legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
