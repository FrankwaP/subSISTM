{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from sklearn.metrics import root_mean_squared_error \n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class: RNN\n",
    "class RNN(nn.Module):\n",
    "    #sizes are the number of nodes for respective layers, as ints\n",
    "    #input_size should be the numbers of variables in input, output_size the number of variables predicted\n",
    "    #hidden_size should be chosen after experimentation, since we only have one layer it should be more than the number of variables\n",
    "    def __init__(self, input_size, hidden_size, output_size, **kwds):\n",
    "        super().__init__(**kwds)\n",
    "        #Number of nodes of the hidden layer (used for init)\n",
    "        self.hidden_size = hidden_size\n",
    "        #Weights\n",
    "        self.i2h = nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    #Input must be torch.Tensor and normalized\n",
    "    def forward(self, input, h0):\n",
    "        #h0 = self.initHidden()\n",
    "        h_list, hn = self.i2h(input, h0)\n",
    "        output_list = self.h2o(h_list)\n",
    "        return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class : LSTM\n",
    "class LSTM(nn.Module):\n",
    "    #sizes are the number of nodes for respective layers, as ints\n",
    "    #input_size should be the numbers of variables in input, output_size the number of variables predicted\n",
    "    #hidden_size should be chosen after experimentation, since we only have one layer it should be more than the number of variables\n",
    "    def __init__(self, input_size, hidden_size, output_size, **kwds):\n",
    "        super().__init__(**kwds)\n",
    "        #Number of nodes of the hidden layer (used for init)\n",
    "        self.hidden_size = hidden_size\n",
    "        #Weights\n",
    "        self.i2h = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    \n",
    "    #Input must be torch.Tensor and normalized\n",
    "    def forward(self, input, h0, c0):\n",
    "        h_list, (hn,cn) = self.i2h(input, (h0, c0))\n",
    "        output_list = self.h2o(h_list)\n",
    "        return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import et preprocessing\n",
    "path = \"C:/Users/Utilisateur/Documents/ENSAE/Stage/simulation/simulation.csv\"\n",
    "dataset = pd.read_csv(path)\n",
    "dataset['y+1'] = dataset.groupby(\"individus\")['y_obs'].shift(-1)\n",
    "dataset = dataset[dataset['temps']%5 == 0]\n",
    "dataset_norm = dataset.copy()\n",
    "dataset_norm = dataset_norm.dropna()\n",
    "individus = dataset_norm.groupby('individus')[['x1_obs', 'x2_obs', 'x3_obs', 'x4_obs', 'x5_obs', \"x6_obs\", 'x7_obs', 'y_obs', 'y+1']]\n",
    "min, max = individus.transform('min'), individus.transform('max')\n",
    "dataset_norm[['x1_obs', 'x2_obs', 'x3_obs', 'x4_obs', 'x5_obs', \"x6_obs\", 'x7_obs', 'y_obs', 'y+1']] = (dataset_norm[['x1_obs', 'x2_obs', 'x3_obs', 'x4_obs', 'x5_obs', \"x6_obs\", 'x7_obs', 'y_obs', 'y+1']] - min) / (max - min)\n",
    "\n",
    "data_train = dataset_norm[dataset_norm['individus']<=400]\n",
    "data_test = dataset_norm[dataset_norm['individus']>400]\n",
    "\n",
    "groupby = data_train.groupby('individus')[['x1_obs', 'x2_obs', 'x3_obs', 'x4_obs', 'x5_obs', \"x6_obs\", 'x7_obs', 'x8', 'y_obs']].apply(np.array)\n",
    "input_train = [torch.Tensor(x) for x in groupby]\n",
    "input_train = torch.stack(input_train)\n",
    "groupby = data_train.groupby('individus')[['y+1']].apply(np.array)\n",
    "target_train = [torch.Tensor(x) for x in groupby]\n",
    "target_train = torch.stack(target_train)\n",
    "\n",
    "groupby = data_test.groupby('individus')[['x1_obs', 'x2_obs', 'x3_obs', 'x4_obs', 'x5_obs', \"x6_obs\", 'x7_obs', 'x8', 'y_obs']].apply(np.array)\n",
    "input_test = [torch.Tensor(x) for x in groupby]\n",
    "input_test = torch.stack(input_test)\n",
    "groupby = data_test.groupby('individus')[['y+1']].apply(np.array)\n",
    "target_test = [torch.Tensor(x) for x in groupby]\n",
    "target_test = torch.stack(target_test)\n",
    "\n",
    "#seq_lens = [len(inp) for inp in groupby]\n",
    "#input_padded = pad_sequence([torch.tensor(x) for x in groupby])\n",
    "#input_pack_padded = pack_padded_sequence(input_padded, lengths=seq_lens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparamètres\n",
    "rnn = RNN(input_size = 9, hidden_size = 25, output_size=1)\n",
    "criterion = torch.nn.MSELoss()\n",
    "#SGD = Stochastic gradient descent, lr = learning rate\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr = 0.01)\n",
    "epoch_rnn = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN: training\n",
    "loss_track = []\n",
    "for k in range(epoch_rnn):\n",
    "    h0 = torch.zeros((1,400,25))\n",
    "    y_pred = rnn(input=input_train, h0=h0)\n",
    "    loss = criterion(y_pred, target_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_track.append(loss.item())\n",
    "print(\"mean RMSE on train data: \", np.mean(np.sqrt(loss_track)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss rnn\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(loss_track, label =\"loss\")\n",
    "plt.legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN : plot 5 random curves from training data \n",
    "n = 5\n",
    "ex = randint(1,400, n)\n",
    "fig, axs = plt.subplots(n)\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(50)\n",
    "for k in range(n):\n",
    "    axs[k].plot(target_train[ex[k]].detach().numpy(), label= 'target')\n",
    "    axs[k].plot(y_pred[ex[k]].detach().numpy(), label= 'prediction')\n",
    "    axs[k].legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN: RMSE on test data\n",
    "h0 = torch.zeros((1,100, 25))\n",
    "y_pred = rnn(input_test, h0)\n",
    "rmse_moy = 0\n",
    "for k in range(len(y_pred)):\n",
    "    rmse_moy+= root_mean_squared_error(y_pred[k].detach().numpy(), target_test[k].detach().numpy())\n",
    "rmse_moy = rmse_moy/len(y_pred)\n",
    "print(\"mean RMSE on test data: \", rmse_moy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN : plot 10 curves from test data\n",
    "n = 10\n",
    "ex = randint(1,100, n)\n",
    "fig, axs = plt.subplots(n)\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(50)\n",
    "res = []\n",
    "for k in range(n):\n",
    "    h0 = torch.zeros((1,25))\n",
    "    y_pred = rnn(input_test[ex[k]], h0)\n",
    "    res.append((ex[k], root_mean_squared_error(target_test[ex[k]].detach().numpy(), y_pred.detach().numpy())))\n",
    "    axs[k].plot(target_test[ex[k]].detach().numpy(), label= 'target')\n",
    "    axs[k].plot(y_pred.detach().numpy(), label= 'prediction')\n",
    "    axs[k].legend()\n",
    "print(res) #res contient les RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparamètres\n",
    "lstm = LSTM(input_size = 9, hidden_size = 25, output_size=1)\n",
    "criterion = torch.nn.MSELoss()\n",
    "#SGD = Stochastic gradient descent, lr = learning rate\n",
    "optimizer = torch.optim.SGD(lstm.parameters(), lr = 0.01)\n",
    "epoch_lstm = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM: training\n",
    "loss_track = []\n",
    "for k in range(epoch_lstm):\n",
    "    h0 = torch.zeros((1,400,25))\n",
    "    c0 = torch.zeros((1,400,25)) \n",
    "    y_pred = lstm(input=input_train, h0=h0, c0 = c0)\n",
    "    loss = criterion(y_pred, target_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_track.append(loss.item())\n",
    "print(\"mean RMSE on train data: \", np.mean(np.sqrt(loss_track)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss lstm\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(loss_track, label =\"loss\")\n",
    "plt.legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM: RMSE on test data\n",
    "h0 = torch.zeros((1,100, 25))\n",
    "c0 = torch.zeros((1,100, 25))\n",
    "y_pred = lstm(input_test, h0, c0)\n",
    "rmse_moy = 0\n",
    "for k in range(len(y_pred)):\n",
    "    rmse_moy+= root_mean_squared_error(y_pred[k].detach().numpy(), target_test[k].detach().numpy())\n",
    "rmse_moy = rmse_moy/len(y_pred)\n",
    "print(\"mean RMSE on test data: \", rmse_moy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM : plot 10 curves from test data\n",
    "n = 10\n",
    "ex = randint(1,100, n)\n",
    "fig, axs = plt.subplots(n)\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(50)\n",
    "res = []\n",
    "for k in range(n):\n",
    "    h0 = torch.zeros((1,25))\n",
    "    c0 = torch.zeros((1,25))\n",
    "    y_pred = lstm(input_test[ex[k]], h0, c0)\n",
    "    res.append((ex[k], root_mean_squared_error(target_test[ex[k]].detach().numpy(), y_pred.detach().numpy())))\n",
    "    axs[k].plot(target_test[ex[k]].detach().numpy(), label= 'target')\n",
    "    axs[k].plot(y_pred.detach().numpy(), label= 'prediction')\n",
    "    axs[k].legend()\n",
    "print(res) #res contient les RMSE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
